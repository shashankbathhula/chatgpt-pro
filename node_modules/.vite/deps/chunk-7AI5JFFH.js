import {
  BaseChain
} from "./chunk-V4JJ2BAT.js";
import {
  BaseLanguageModel
} from "./chunk-4FBMTBSL.js";
import {
  BasePromptTemplate,
  Runnable
} from "./chunk-AVBPOKY3.js";

// node_modules/langchain/dist/schema/output_parser.js
var BaseLLMOutputParser = class extends Runnable {
  /**
   * Parses the result of an LLM call with a given prompt. By default, it
   * simply calls `parseResult`.
   * @param generations The generations from an LLM call.
   * @param _prompt The prompt used in the LLM call.
   * @param callbacks Optional callbacks.
   * @returns A promise of the parsed output.
   */
  parseResultWithPrompt(generations, _prompt, callbacks) {
    return this.parseResult(generations, callbacks);
  }
  /**
   * Calls the parser with a given input and optional configuration options.
   * If the input is a string, it creates a generation with the input as
   * text and calls `parseResult`. If the input is a `BaseMessage`, it
   * creates a generation with the input as a message and the content of the
   * input as text, and then calls `parseResult`.
   * @param input The input to the parser, which can be a string or a `BaseMessage`.
   * @param options Optional configuration options.
   * @returns A promise of the parsed output.
   */
  async invoke(input, options) {
    if (typeof input === "string") {
      return this._callWithConfig(async (input2) => this.parseResult([{ text: input2 }]), input, { ...options, runType: "parser" });
    } else {
      return this._callWithConfig(async (input2) => this.parseResult([{ message: input2, text: input2.content }]), input, { ...options, runType: "parser" });
    }
  }
};
var BaseOutputParser = class extends BaseLLMOutputParser {
  parseResult(generations, callbacks) {
    return this.parse(generations[0].text, callbacks);
  }
  async parseWithPrompt(text, _prompt, callbacks) {
    return this.parse(text, callbacks);
  }
  /**
   * Return the string type key uniquely identifying this class of parser
   */
  _type() {
    throw new Error("_type not implemented");
  }
};
var OutputParserException = class extends Error {
  constructor(message, llmOutput, observation, sendToLLM = false) {
    super(message);
    Object.defineProperty(this, "llmOutput", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "observation", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "sendToLLM", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    this.llmOutput = llmOutput;
    this.observation = observation;
    this.sendToLLM = sendToLLM;
    if (sendToLLM) {
      if (observation === void 0 || llmOutput === void 0) {
        throw new Error("Arguments 'observation' & 'llmOutput' are required if 'sendToLlm' is true");
      }
    }
  }
};

// node_modules/langchain/dist/output_parsers/noop.js
var NoOpOutputParser = class extends BaseOutputParser {
  constructor() {
    super(...arguments);
    Object.defineProperty(this, "lc_namespace", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: ["langchain", "output_parsers", "default"]
    });
    Object.defineProperty(this, "lc_serializable", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: true
    });
  }
  static lc_name() {
    return "NoOpOutputParser";
  }
  /**
   * This method takes a string as input and returns the same string as
   * output. It does not perform any operations on the input string.
   * @param text The input string to be parsed.
   * @returns The same input string without any operations performed on it.
   */
  parse(text) {
    return Promise.resolve(text);
  }
  /**
   * This method returns an empty string. It does not provide any formatting
   * instructions.
   * @returns An empty string, indicating no formatting instructions.
   */
  getFormatInstructions() {
    return "";
  }
};

// node_modules/langchain/dist/chains/llm_chain.js
var LLMChain = class _LLMChain extends BaseChain {
  static lc_name() {
    return "LLMChain";
  }
  get inputKeys() {
    return this.prompt.inputVariables;
  }
  get outputKeys() {
    return [this.outputKey];
  }
  constructor(fields) {
    super(fields);
    Object.defineProperty(this, "lc_serializable", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: true
    });
    Object.defineProperty(this, "prompt", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "llm", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "llmKwargs", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "outputKey", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: "text"
    });
    Object.defineProperty(this, "outputParser", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    this.prompt = fields.prompt;
    this.llm = fields.llm;
    this.llmKwargs = fields.llmKwargs;
    this.outputKey = fields.outputKey ?? this.outputKey;
    this.outputParser = fields.outputParser ?? new NoOpOutputParser();
    if (this.prompt.outputParser) {
      if (fields.outputParser) {
        throw new Error("Cannot set both outputParser and prompt.outputParser");
      }
      this.outputParser = this.prompt.outputParser;
    }
  }
  /** @ignore */
  _selectMemoryInputs(values) {
    const valuesForMemory = super._selectMemoryInputs(values);
    for (const key of this.llm.callKeys) {
      if (key in values) {
        delete valuesForMemory[key];
      }
    }
    return valuesForMemory;
  }
  /** @ignore */
  async _getFinalOutput(generations, promptValue, runManager) {
    let finalCompletion;
    if (this.outputParser) {
      finalCompletion = await this.outputParser.parseResultWithPrompt(generations, promptValue, runManager == null ? void 0 : runManager.getChild());
    } else {
      finalCompletion = generations[0].text;
    }
    return finalCompletion;
  }
  /**
   * Run the core logic of this chain and add to output if desired.
   *
   * Wraps _call and handles memory.
   */
  call(values, config) {
    return super.call(values, config);
  }
  /** @ignore */
  async _call(values, runManager) {
    const valuesForPrompt = { ...values };
    const valuesForLLM = {
      ...this.llmKwargs
    };
    for (const key of this.llm.callKeys) {
      if (key in values) {
        valuesForLLM[key] = values[key];
        delete valuesForPrompt[key];
      }
    }
    const promptValue = await this.prompt.formatPromptValue(valuesForPrompt);
    const { generations } = await this.llm.generatePrompt([promptValue], valuesForLLM, runManager == null ? void 0 : runManager.getChild());
    return {
      [this.outputKey]: await this._getFinalOutput(generations[0], promptValue, runManager)
    };
  }
  /**
   * Format prompt with values and pass to LLM
   *
   * @param values - keys to pass to prompt template
   * @param callbackManager - CallbackManager to use
   * @returns Completion from LLM.
   *
   * @example
   * ```ts
   * llm.predict({ adjective: "funny" })
   * ```
   */
  async predict(values, callbackManager) {
    const output = await this.call(values, callbackManager);
    return output[this.outputKey];
  }
  _chainType() {
    return "llm";
  }
  static async deserialize(data) {
    const { llm, prompt } = data;
    if (!llm) {
      throw new Error("LLMChain must have llm");
    }
    if (!prompt) {
      throw new Error("LLMChain must have prompt");
    }
    return new _LLMChain({
      llm: await BaseLanguageModel.deserialize(llm),
      prompt: await BasePromptTemplate.deserialize(prompt)
    });
  }
  /** @deprecated */
  serialize() {
    return {
      _type: `${this._chainType()}_chain`,
      llm: this.llm.serialize(),
      prompt: this.prompt.serialize()
    };
  }
};

export {
  BaseLLMOutputParser,
  BaseOutputParser,
  OutputParserException,
  LLMChain
};
//# sourceMappingURL=chunk-7AI5JFFH.js.map
